#ifndef CRYPTO_ONETIMEAUTH_POLY1305_REF3
#define CRYPTO_ONETIMEAUTH_POLY1305_REF3

// little endian for loads and stores
inline fn __load2(reg u64 p) -> reg u64[2]
{
  reg u64[2] x;
  x[0] = [p + 0];
  x[1] = [p + 8];
  return x;
}



inline fn __load_add(reg u64[3] h, reg u64 in) -> reg u64[3]
{
  reg bool cf;
  cf, h[0] += [in + 0];
  cf, h[1] += [in + 8] + cf;
   _, h[2] +=        1 + cf;
  return h;
}


#if 0
inline fn __load_last_add(reg u64[3] h, reg u64 in, reg u64 len) -> reg u64[3]
{
  reg bool cf;
  reg   u64    j;
  stack u64[2] s;
  reg   u8     c;

  s[0] = 0;
  s[1] = 0;

  #if 0
  j = 0;
  while(j < len)
  { c = (u8)[in + j];
    s[u8 (int)j] = c;
    j += 1;
  }
  #endif

  #if 1
  j = 0;
  while(j < len)
  { c = (u8)[in];
    s[u8 (int)j] = c;
    in += 1;
    j += 1;
  }
  #endif

  s[u8 (int)j] = 0x1;

  cf, h[0] += s[0];
  cf, h[1] += s[1] + cf;
   _, h[2] +=    0 + cf;

  return h;
}
#else

// @pre: 0 <= len <= 15
inline fn __load_last_add_mask(reg u64 len) -> reg u64[2], reg u64[2]
{
  reg bool cf;
  reg   u64 s0 s1 s2;
  reg   u64[2] m1 m2;

  s0 = 15;
  s0 -= len; // [0-15] -> [15-0]
  s0 <<= 3;

  s1  =  s0;
  s1 &=  7*8;  // first 3 bits s1 in [0-7]*8
  s2  =  s0;
  s2 &=  -8*8; // bit 4 s2 in [0,8]*8
  s0  =  s2;

  s0 >>= 4;    // avoid s's being 64
  s1 += s0;
  s2 -= s0;

  m1[0] = -1;
  m1[1] = (1 << 64 - 8) - 1;

  _,_,_,_,_, m1[0] = #SHRD(m1[0], m1[1], s1);
  m1[1] >>= s1;
  s1   = s2; // rcx
  _,_,_,_,_, m1[0] = #SHRD(m1[0], m1[1], s1);
  m1[1] >>= s1;

  m2 = m1;

  cf, m2[0] += 1;
   _, m2[1] += 0 + cf;

  return m1, m2;
}

inline fn __load_last_add(reg u64[3] h, reg u64 in len) -> reg u64[3]
{
  inline int i;
  reg bool cf;
  reg   u64[2] v m1 m2;

  m1, m2 = __load_last_add_mask(len);

  for i=0 to 2
  { v[i] = [in + 8*i];
    v[i] &= m1[i];
    v[i] |= m2[i];
  }

  cf, h[0] += v[0];
  cf, h[1] += v[1] + cf;
   _, h[2] +=    0 + cf;
 
  return h;
}
#endif



inline fn __store2(reg u64 p, reg u64[2] x)
{
  [p + 0] = x[0];
  [p + 8] = x[1];
}



inline fn __clamp(reg u64 k) -> reg u64[3]
{
  reg u64[3] r;
  r[0] = [k + 0];
  r[1] = [k + 8];
  r[0] &= 0x0ffffffc0fffffff;
  r[1] &= 0x0ffffffc0ffffffc;
  r[2] = r[1];
  r[2] >>= 2;
  r[2] += r[1];
  return r;
}



// h += s
inline fn __add2(reg u64[2] h s) -> reg u64[2]
{
  reg bool cf;
  cf, h[0] += s[0];
   _, h[1] += s[1] + cf;
  return h;
}



inline fn __mulmod(reg u64[3] h r) -> reg u64[3]
{
  reg bool cf;
  reg u64 t0 t1 t2;
  reg u64 rax rdx;

  t2 = r[2];
  t2 *= h[2];     // (* t2 = h[2]*r[2] *)
  h[2] *= r[0];   // (* h[2] = h[2]*r[0] *)

  rax = r[0];
  rdx, rax = rax * h[0];
  t0 = rax;      // (* t0 = h[0]*r[0] *)
  t1 = rdx;      // (* t1 = mulhi h[0] r[0] *)

  rax = r[0];
  rdx, rax = rax * h[1];

  cf, t1 += rax;           // (* t1 = h[1]*r[0] + mulhi h[0] r[0]*)
  _ , h[2] += rdx + cf;    // (* h[2] = h[2]*r[0] + mulhi h[1] r[0] + CF *)

  rax = r[2];
  rdx, rax = rax * h[1];
  h[1] = rdx;
  h[1] += t2;    // (* h[1] = h[2]*r[2] + mulhi h[1] r[2] *)
  t2 = rax;      // (* t2 = h[1]*r[2] *)

  rax = r[1];
  rdx, rax = rax * h[0];

  cf, t0 += t2;            // (* t0 = h[0]*r[0] + h[1]*r[2] *)
  cf, t1 += rax + cf;      // (* t1 = h[0]*r[1] + t1 + CF *)
  _ , h[2] += rdx + cf;    // (* h[2] = mulhi h[0] r[1] + h[2] + CF *)

  h[0] = 0xfffffffffffffffc;
    t2 = h[2]; 
    t2 >>= 2;
  h[0] &= h[2];

  h[0] += t2;
  h[2] &= 0x03;

  cf, h[0] += t0;
  cf, h[1] += t1 + cf;
  _ , h[2] +=  0 + cf;

  return h;
}



inline fn __freeze(reg u64[3] h) -> reg u64[2]
{
  reg bool cf;
  reg u64[2] g;
  reg u64 g2;
  reg u64 mask;

  g[0] = h[0];
  g[1] = h[1];
  g2 = h[2];

  //                  <= 6 then g[2] can be at most 7 (111b)
  // if h[2] value is <= 4 then g[2] can be at most 5 (101b)
  cf, g[0] += 5;
  cf, g[1] += 0 + cf;
   _, g2 += 0 + cf;

  // which means that by shifting right by 2 we are left with only 1 bit set
  g2 >>= 2;

  // and if this bit is set g[2]: mask will be 2**64-1 (all bits are set) otherwise
  // the mask will be zero
  mask = -g2;

  g[0] ^= h[0];
  g[1] ^= h[1];

  g[0] &= mask;
  g[1] &= mask;

  // if bit == 1 then h[0..1] ^= (g[0..1] ^ h[0..1])
  // else             h[0..1] ^= 0
  g[0] ^= h[0];
  g[1] ^= h[1];

  // at this point we only need the first 128 bits
  return g;
}



inline fn __poly1305_ref3_setup(reg u64 k) -> reg u64[3], reg u64[3], reg u64
{
  inline int i;
  reg u64[3] h;
  reg u64[3] r;
  reg u64 len;

  for i=0 to 3 { h[i] = 0; }
  r = __clamp(k);
  k += 16;
  return h, r, k;
}



inline fn __poly1305_ref3_update(reg u64 in inlen, reg u64[3] h r) -> reg u64, reg u64, reg u64[3]
{
  reg bool cf;
  reg u64[2] m;

  while(inlen >= 16)
  {
    h = __load_add(h, in);
    h = __mulmod(h, r);
    in += 16;
    inlen -= 16;
  }

  return in, inlen, h;
}



inline fn __poly1305_ref3_last(reg u64 out in inlen k, reg u64[3] h r)
{
  reg u64[2] m s h2;

  if(inlen > 0)
  { h = __load_last_add(h, in, inlen);
    h = __mulmod(h, r);
  }

  h2 = __freeze(h);
  s = __load2(k);
  h2 = __add2(h2, s);

  __store2(out, h2);
}



inline fn __poly1305_ref3(reg u64 out in inlen k)
{
  reg u64[3] h r;

  h, r, k = __poly1305_ref3_setup(k);
  in, inlen, h = __poly1305_ref3_update(in, inlen, h, r);
  __poly1305_ref3_last(out, in, inlen, k, h, r);
}



#ifdef EXPORT

export fn poly1305_ref3(reg u64 out in _inlen _k)
{
  reg u64 inlen k;

  inlen = _inlen;
  k = _k;
  __poly1305_ref3(out, in, inlen, k);
}

#endif
#endif

