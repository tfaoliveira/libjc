
//require "crypto_stream/chacha/avx/chacha_globals.jinc"
require "chacha_globals.jinc"

///////////////////////////////////////////////////////////////////////////////
// 'core' code for 1 block (64 bytes) /////////////////////////////////////////


inline fn __copy_state_h_avx(reg u128[4] st) -> reg u128[4]
{
  reg u128[4] k;
  k = #copy(st);
  return k;
}


inline fn __rotate_h_avx(reg u128[4] k, inline int i r, reg u128 r16 r8) -> reg u128[4]
{
  reg u128 t;

  if(r==16){
    k[i] = #VPSHUFB_128(k[i], r16);

  } else { if (r==8) {
    k[i] = #VPSHUFB_128(k[i], r8);

  } else {
    t = k[i] <<4u32 r;
    k[i] = k[i] >>4u32 (32-r);
    k[i] ^= t;

  }}

  return k;
}


inline fn __line_h_avx(reg u128[4] k, inline int a b c r, reg u128 r16 r8) -> reg u128[4]
{
  k[a] +4u32= k[b];
  k[c] ^= k[a];
  k = __rotate_h_avx(k, c, r, r16, r8);
  return k;
}


inline fn __round_h_avx(reg u128[4] k, reg u128 r16 r8) -> reg u128[4]
{
  k = __line_h_avx(k, 0, 1, 3,  16, r16, r8);
  k = __line_h_avx(k, 2, 3, 1,  12, r16, r8);
  k = __line_h_avx(k, 0, 1, 3,   8, r16, r8);
  k = __line_h_avx(k, 2, 3, 1,   7, r16, r8);
  return k;
}


inline fn __shuffle_state_h_avx(reg u128[4] k) -> reg u128[4]
{
  k[1] = #VPSHUFD_128(k[1], (4u2)[0,3,2,1]);
  k[2] = #VPSHUFD_128(k[2], (4u2)[1,0,3,2]);
  k[3] = #VPSHUFD_128(k[3], (4u2)[2,1,0,3]);
  return k;
}


inline fn __reverse_shuffle_state_h_avx(reg u128[4] k) -> reg u128[4]
{
  k[1] = #VPSHUFD_128(k[1], (4u2)[2,1,0,3]);
  k[2] = #VPSHUFD_128(k[2], (4u2)[1,0,3,2]);
  k[3] = #VPSHUFD_128(k[3], (4u2)[0,3,2,1]);
  return k;
}


inline fn __column_round_h_avx(reg u128[4] k, reg u128 r16 r8) -> reg u128[4]
{
  k = __round_h_avx(k, r16, r8);
  return k;
}


inline fn __diagonal_round_h_avx(reg u128[4] k, reg u128 r16 r8) -> reg u128[4]
{
  k = __shuffle_state_h_avx(k);
  k = __round_h_avx(k, r16, r8);
  k = __reverse_shuffle_state_h_avx(k);
  return k;
}


inline fn __double_round_h_avx(reg u128[4] k, reg u128 r16 r8) -> reg u128[4]
{
  k = __column_round_h_avx(k, r16, r8);
  k = __diagonal_round_h_avx(k, r16, r8);
  return k;
}


inline fn __rounds_h_avx(reg u128[4] k, reg u128 r16 r8) -> reg u128[4]
{
  reg u32 c;

  c = (CHACHA_ROUNDS/2);
  while
  {
    k = __double_round_h_avx(k, r16, r8);
    (_,_,_,_,c) = #DEC_32(c);
  } (c > 0)

  return k;
}


inline fn __sum_states_h_avx(reg u128[4] k st) -> reg u128[4]
{
  inline int i;
  for i=0 to 4
  { k[i] +4u32= st[i]; }
  return k;
}


///////////////////////////////////////////////////////////////////////////////
// 'core' code for 2 blocks (128 bytes) ///////////////////////////////////////


inline fn __copy_state_h_x2_avx(reg u128[4] st) -> reg u128[4], reg u128[4]
{
  reg u128[4] k1 k2;
  k1 = #copy(st);
  k2 = #copy(st);
  k2 = __increment_counter01_h_avx(k2);
  return k1, k2;
}


inline fn __round_h_x2_inline_avx(reg u128[4] k1 k2, reg u128 r16 r8) -> reg u128[4], reg u128[4]
{
  reg u128 t1;

  k1[0] +4u32= k1[1];
  k2[0] +4u32= k2[1];

  k1[3] ^= k1[0];
  k2[3] ^= k2[0];

  k1 = __rotate_h_avx(k1, 3, 16, r16, r8);
  k2 = __rotate_h_avx(k2, 3, 16, r16, r8);


  k1[2] +4u32= k1[3];
  k2[2] +4u32= k2[3];

                          k1[1] ^= k1[2];
  // inlined rotate
  t1 = k1[1] <<4u32 12;
  k1[1] = k1[1] >>4u32 20;
                          k2[1] ^= k2[2];
  k1[1] ^= t1;

  k2 = __rotate_h_avx(k2, 1, 12, r16, r8);

  k1[0] +4u32= k1[1];
  k2[0] +4u32= k2[1];

  k1[3] ^= k1[0];
  k2[3] ^= k2[0];

  k1 = __rotate_h_avx(k1, 3, 8, r16, r8);
  k2 = __rotate_h_avx(k2, 3, 8, r16, r8);

  k1[2] +4u32= k1[3];
  k2[2] +4u32= k2[3];

                          k1[1] ^= k1[2];
  // inlined rotate
  t1 = k1[1] <<4u32 7;
  k1[1] = k1[1] >>4u32 25;
                          k2[1] ^= k2[2];
  k1[1] ^= t1;

  k2 = __rotate_h_avx(k2, 1, 7, r16, r8);

  return k1, k2;
}


inline fn __shuffle_state_h_x2_avx(reg u128[4] k1 k2) -> reg u128[4], reg u128[4]
{
  k1 = __shuffle_state_h_avx(k1);
  k2 = __shuffle_state_h_avx(k2);
  return k1, k2;
}


inline fn __reverse_shuffle_state_h_x2_avx(reg u128[4] k1 k2) -> reg u128[4], reg u128[4]
{
  k1 = __reverse_shuffle_state_h_avx(k1);
  k2 = __reverse_shuffle_state_h_avx(k2);
  return k1, k2;
}


inline fn __column_round_h_x2_avx(reg u128[4] k1 k2, reg u128 r16 r8) -> reg u128[4], reg u128[4]
{
  k1, k2 = __round_h_x2_inline_avx(k1, k2, r16, r8);
  return k1, k2;
}


inline fn __diagonal_round_h_x2_avx(reg u128[4] k1 k2, reg u128 r16 r8) -> reg u128[4], reg u128[4]
{
  k1, k2 = __shuffle_state_h_x2_avx(k1, k2);
  k1, k2 = __round_h_x2_inline_avx(k1, k2, r16, r8);
  k1, k2 = __reverse_shuffle_state_h_x2_avx(k1, k2);
  return k1, k2;
}


inline fn __double_round_h_x2_avx(reg u128[4] k1 k2, reg u128 r16 r8) -> reg u128[4], reg u128[4]
{
  k1, k2 = __column_round_h_x2_avx(k1, k2, r16, r8);
  k1, k2 = __diagonal_round_h_x2_avx(k1, k2, r16, r8);
  return k1, k2;
}


inline fn __rounds_h_x2_avx(reg u128[4] k1 k2, reg u128 r16 r8) -> reg u128[4], reg u128[4]
{
  reg u32 c;

  c = (CHACHA_ROUNDS/2);
  while
  {
    k1, k2 = __double_round_h_x2_avx(k1, k2, r16, r8);
    (_,_,_,_,c) = #DEC_32(c);
  } (c > 0)

  return k1, k2;
}


inline fn __sum_states_h_x2_avx(reg u128[4] k1 k2 st) -> reg u128[4], reg u128[4]
{
  k1 = __sum_states_h_avx(k1, st);
  k2 = __sum_states_h_avx(k2, st);
  k2 = __increment_counter01_h_avx(k2);
  return k1, k2;
}


///////////////////////////////////////////////////////////////////////////////


inline fn __chacha_h_avx(reg u64 output plain len nonce key)
{
  reg u128[4] st k;
  reg u128 r16 r8;

  r16 = CHACHA_R16_AVX;
  r8  = CHACHA_R8_AVX;
  st = __init_h_avx(nonce, key);

  while(len >= 64)
  {
    k = __copy_state_h_avx(st);
    k = __rounds_h_avx(k, r16, r8);
    k = __sum_states_h_avx(k, st);
    output, plain, len = __store_h_avx(output, plain, len, k);
    st = __increment_counter01_h_avx(st);
  }

  if(len > 0)
  { k = __copy_state_h_avx(st);
    k = __rounds_h_avx(k, r16, r8);
    k = __sum_states_h_avx(k, st);
    __store_last_h_avx(output, plain, len, k);
  }
}


inline fn __chacha_h_x2_avx(reg u64 output plain len nonce key)
{
  reg u128[4] st k1 k2;
  reg u128 r16 r8;

  r16 = CHACHA_R16_AVX;
  r8  = CHACHA_R8_AVX;
  st = __init_h_avx(nonce, key);

  while(len >= 128)
  {
    k1, k2 = __copy_state_h_x2_avx(st);
    k1, k2 = __rounds_h_x2_avx(k1, k2, r16, r8);
    k1, k2 = __sum_states_h_x2_avx(k1, k2, st);
    output, plain, len = __store_h_x2_avx(output, plain, len, k1, k2);
    st = __increment_counter02_h_avx(st);
  }

  if(len > 64)
  { k1, k2 = __copy_state_h_x2_avx(st);
    k1, k2 = __rounds_h_x2_avx(k1, k2, r16, r8);
    k1, k2 = __sum_states_h_x2_avx(k1, k2, st);
    output, plain, len = __store_h_avx(output, plain, len, k1);
    __store_last_h_avx(output, plain, len, k2);
  }
  else
  { k1 = __copy_state_h_avx(st);
    k1 = __rounds_h_avx(k1, r16, r8);
    k1 = __sum_states_h_avx(k1, st);
    __store_last_h_avx(output, plain, len, k1);
  }
}


