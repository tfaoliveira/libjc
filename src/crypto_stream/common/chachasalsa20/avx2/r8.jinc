#ifndef CRYPTO_STREAM_COMMON_CHACHASALSA20_R8_AVX2
#define CRYPTO_STREAM_COMMON_CHACHASALSA20_R8_AVX2

// code taken from chacha20.jazz avx2 implementation:
// - clean up and refactor and then include in chacha20
// - to make this generic it may be necessary to move some code into r2/r4.jinc

param int i_0  = 0;
param int i_4  = 4;
param int i_32 = 32;

// store auxiliary functions
inline fn __update_ptr_avx2(reg u64 output plain, reg u64 len, inline int n) -> reg u64, reg u64, reg u64
{
  output += n;
  plain += n;
  len -= n;
  return output, plain, len;
}

// stores 64 bytes
inline fn __store_avx2(reg u64 output plain, reg u64 len, reg u256[2] k) -> reg u64, reg u64, reg u64, reg u256[2]
{
  k[0] ^= (u256)[plain +  0];
  k[1] ^= (u256)[plain + 32];

  (u256)[output +  0] = k[0];
  (u256)[output + 32] = k[1];

  output, plain, len = __update_ptr_avx2(output, plain, len, 64);

  return output, plain, len, k;
}

// stores up to 64 bytes
inline fn __store_last_avx2(reg u64 output plain, reg u64 len, reg u256[2] k)
{
  reg u256     r0;
  reg u128     r1;
  reg u64      r2 j;
  reg u8       r3;
  stack u8[16] s0; 

  r0 = k[0];

  if(len >= 32)
  {
    r0 ^= (u256)[plain + 0];
    (u256)[output + 0] = r0;

    output, plain, len = __update_ptr_avx2(output, plain, len, 32);

    r0 = k[1];
  }

  r1 = #VEXTRACTI128(r0, 0);

  if(len >= 16)
  {
    r1 ^= (u128)[plain + 0];
    (u128)[output + 0] = r1;

    output, plain, len = __update_ptr_avx2(output, plain, len, 16);

    r1 = #VEXTRACTI128(r0, 1);
  }

  s0[u128 0] = r1;

  j = 0;
  while(j < len)
  {
    r3 = (u8)[plain + j];
    r3 ^= s0[(int)j];
    (u8)[output + j] = r3;
    j += 1;
  }
}

// stores 128 bytes
inline fn __store_x2_avx2(reg u64 output plain, reg u64 len, reg u256[4] k) -> reg u64, reg u64, reg u64, reg u256[4]
{
  inline int i;

  for i=0 to 4
  { k[i] ^= (u256)[plain + 32*i]; }

  for i=0 to 4
  { (u256)[output + 32*i] = k[i]; }

  output, plain, len = __update_ptr_avx2(output, plain, len, 128);

  return output, plain, len, k;
}

// stores up to 128 bytes
inline fn __store_x2_last_avx2(reg u64 output plain, reg u64 len, reg u256[4] k)
{
  reg u256[2] r;

  r[0] = k[0];
  r[1] = k[1];

  if(len >= 64)
  {
    output, plain, len, r = __store_avx2(output, plain, len, r);
    r[0] = k[2];
    r[1] = k[3];
  }

  __store_last_avx2(output, plain, len, r);
}

// stores 256 bytes
inline fn __store_x4_avx2(reg u64 output plain, reg u64 len, reg u256[8] k) -> reg u64, reg u64, reg u64
{
  inline int i;

  for i=0 to 8
  { k[i] ^= (u256)[plain + 32*i]; }

  for i=0 to 8
  { (u256)[output + 32*i] = k[i]; }

  output, plain, len = __update_ptr_avx2(output, plain, len, 256);

  return output, plain, len;
}

// stores up to 256 bytes
inline fn __store_x4_last_avx2(reg u64 output plain, reg u64 len, reg u256[8] k)
{
  inline int i;
  reg u256[4] r;

  for i=0 to 4 { r[i] = k[i]; }

  if(len >= 128)
  {
    output, plain, len, r = __store_x2_avx2(output, plain, len, r);
    for i=0 to 4 { r[i] = k[i+4]; }
  }

  __store_x2_last_avx2(output, plain, len, r);
}


// stores 512 bytes auxiliary functions
inline fn __store_half_x8_avx2(reg u64 output plain, reg u64 len, reg u256[8] k, inline int o)
{
  inline int i;

  for i=0 to 8
  { k[i] ^= (u256)[plain + o + 64*i]; }
  for i=0 to 8
  { (u256)[output + o + 64*i] = k[i]; }
}


inline fn __sub_rotate_avx2(reg u256[8] t) -> reg u256[8]
{
  inline int i;
  reg u256[8] x;

  x[0] = #VPUNPCKL_4u64(t[0], t[1]);
  x[1] = #VPUNPCKL_4u64(t[2], t[3]);
  x[2] = #VPUNPCKH_4u64(t[0], t[1]);
  x[3] = #VPUNPCKH_4u64(t[2], t[3]);

  x[4] = #VPUNPCKL_4u64(t[4], t[5]);
  x[5] = #VPUNPCKL_4u64(t[6], t[7]);
  x[6] = #VPUNPCKH_4u64(t[4], t[5]);
  x[7] = #VPUNPCKH_4u64(t[6], t[7]);

  for i=0 to 4
  {   t[i] = #VPERM2I128(x[2*i+0], x[2*i+1], (2u4)[2,0]);
    t[i+4] = #VPERM2I128(x[2*i+0], x[2*i+1], (2u4)[3,1]); }

  return t;
}

inline fn __rotate_avx2(reg u256[8] x) -> reg u256[8]
{
  inline int i;
  reg u256[8] t;

  for i=0 to 4
  {   t[i] = #VPUNPCKL_8u32(x[2*i+0], x[2*i+1]);
    t[i+4] = #VPUNPCKH_8u32(x[2*i+0], x[2*i+1]); }

  t = __sub_rotate_avx2(t);

  return t;
}

inline fn __rotate_stack_avx2(stack u256[8] s) -> reg u256[8]
{
  inline int i;
  reg u256[8] t x;

  for i=0 to 4
  { x[i] = s[2*i+0]; }

  for i=0 to 4
  { t[  i] = #VPUNPCKL_8u32(x[i], s[2*i+1]);
    t[4+i] = #VPUNPCKH_8u32(x[i], s[2*i+1]); }

  t = __sub_rotate_avx2(t);

  return t;
}

inline fn __rotate_first_half_x8_avx2(reg u256[16] k) -> reg u256[8], stack u256[8]
{
  inline int i;
  stack u256[8] s_k8_15;
  reg   u256[8] k0_7;

  for i=0 to 8
  { s_k8_15[i] = k[8+i]; }

  for i=0 to 8
  { k0_7[i] = k[i]; }

  k0_7 = __rotate_avx2(k0_7);

  return k0_7, s_k8_15;
}

inline fn __rotate_second_half_x8_avx2(stack u256[8] s_k8_15) -> reg u256[8]
{
  inline int i;
  reg u256[8] k8_15;
  k8_15 = __rotate_stack_avx2(s_k8_15);
  return k8_15;
}

inline fn __interleave_avx2(stack u256[8] s, reg u256[8] k, inline int o) -> reg u256[8]
{
  inline int i;
  reg u256[8] sk;

  for i=0 to 4
  { sk[2*i+0] = s[o + i];
    sk[2*i+1] = k[o + i]; }

  return sk;
}

// stores 512 bytes
inline fn __store_x8_avx2(reg u64 output plain, reg u64 len, reg u256[16] k) -> reg u256[16], reg u64, reg u64, reg u64
{
#if 0
  inline int i;
  for i=0 to 16
  { (u256)[output + 32*i] = k[i]; }
#endif

  inline int i;
  stack u256[8] s_k8_15;
  reg   u256[8] k0_7, k8_15;

  k0_7, s_k8_15 = __rotate_first_half_x8_avx2(k);
  __store_half_x8_avx2(output, plain, len, k0_7, i_0);
  k8_15 = __rotate_second_half_x8_avx2(s_k8_15);
  __store_half_x8_avx2(output, plain, len, k8_15, i_32);

  output, plain, len = __update_ptr_avx2(output, plain, len, 512);
  return k, output, plain, len;
}

// stores up to 512 bytes
inline fn __store_x8_last_avx2(reg u64 output plain, reg u64 len, reg u256[16] k)
{
  inline int i;
  stack u256[8] s_k0_7 s_k8_15;
  reg   u256[8] k0_7 k8_15 i0_7;

  k0_7, s_k8_15 = __rotate_first_half_x8_avx2(k);
  s_k0_7 = k0_7;
  k8_15 = __rotate_second_half_x8_avx2(s_k8_15);
  i0_7 = __interleave_avx2(s_k0_7, k8_15, i_0);

  if(len >= 256)
  {
    output, plain, len = __store_x4_avx2(output, plain, len, i0_7);
    i0_7 = __interleave_avx2(s_k0_7, k8_15, i_4);
  }

  __store_x4_last_avx2(output, plain, len, i0_7);
}

#endif
