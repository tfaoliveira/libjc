u128 g_r16_u128 = (16u8)[13,12,15,14, 9,8,11,10, 5,4,7,6, 1,0,3,2];
u256 g_r16      = (32u8)[13,12,15,14, 9,8,11,10, 5,4,7,6, 1,0,3,2, 13,12,15,14, 9,8,11,10, 5,4,7,6, 1,0,3,2];
u256 g_r8       = (32u8)[14,13,12,15, 10,9,8,11, 6,5,4,7, 2,1,0,3, 14,13,12,15, 10,9,8,11, 6,5,4,7, 2,1,0,3];
u256 g_cnt      = (8u32)[7,6,5,4,3,2,1,0];
u256 g_cnt_inc  = (8u32)[8,8,8,8,8,8,8,8];
u256 g_p1       = (2u128)[1,0];
u256 g_p2       = (2u128)[2,2];

u128 g_sigma   = 0x6b20657479622d323320646e61707865;
u32 g_sigma0   = 0x61707865;
u32 g_sigma1   = 0x3320646e;
u32 g_sigma2   = 0x79622d32;
u32 g_sigma3   = 0x6b206574;
u128 g_p0      = 0;

param int i_0 = 0;
param int i_1 = 1;
param int i_2 = 2;
param int i_3 = 3;
param int i_4 = 4;
param int i_5 = 5;
param int i_6 = 6;
param int i_7 = 7;
param int i_8 = 8;
param int i_9 = 9;
param int i_10 = 10;
param int i_11 = 11;
param int i_12 = 12;
param int i_13 = 13;
param int i_14 = 14;
param int i_15 = 15;
param int i_32 = 32;

fn load_shufb_cmd() -> stack u256, stack u256
{
  reg   u256   r16,   r8;
  stack u256 s_r16, s_r8;

  r16 = g_r16;
  r8 = g_r8;
  s_r16 = r16;
  s_r8 = r8;
  return s_r16, s_r8;
}

fn init_x2(reg u64 key nonce, reg u32 counter) -> reg u256[4]
{
  reg u256[4] st;
  reg u128 nc;
  stack u128 s_nc;

  nc = g_p0;
  nc = #x86_VPINSR_4u32(nc, counter, 0);
  nc = #x86_VPINSR_4u32(nc, (u32)[nonce + 0], 1);
  nc = #x86_VPINSR_2u64(nc, (u64)[nonce + 4], 1);
  s_nc = nc;

  st[0] = #x86_VPBROADCAST_2u128(g_sigma);
  st[1] = #x86_VPBROADCAST_2u128((u128)[key + 0]);
  st[2] = #x86_VPBROADCAST_2u128((u128)[key + 16]);
  st[3] = #x86_VPBROADCAST_2u128(s_nc);
  st[3] +8u32= g_p1;

  // st
  // 0 { sigma     , sigma      }
  // 1 { k[127:0]  , k[127:0]   }
  // 2 { k[255:128], k[255:128] }
  // 3 { n , cnt+1 , n , cnt    }
  return st;
}



fn init_x8(reg u64 key nonce, reg u32 counter) -> stack u256[16]
{
  inline int i;
  stack u256[16] st_; 
  reg u256[16] st;
  stack u32 s_counter;

  s_counter = counter;

  st[0] = #x86_VPBROADCAST_8u32(g_sigma0);
  st[1] = #x86_VPBROADCAST_8u32(g_sigma1);
  st[2] = #x86_VPBROADCAST_8u32(g_sigma2);
  st[3] = #x86_VPBROADCAST_8u32(g_sigma3);

  for i=0 to 8
  { st[i+4] = #x86_VPBROADCAST_8u32((u32)[key + i*4]); }

  st[12] = #x86_VPBROADCAST_8u32(s_counter);
  st[12] +8u32= g_cnt;

  for i=0 to 3
  { st[i+13] = #x86_VPBROADCAST_8u32((u32)[nonce + i*4]); }

  //  st
  //  0 { ... , sigma0     , sigma0     }
  //  1 { ... , sigma1     , sigma1     }
  //  2 { ... , sigma2     , sigma2     }
  //  3 { ... , sigma3     , sigma3     }
  //  4 { ... , k[31:0]    , k[31:0]    }
  // ...
  // 11 { ... , k[255:224] , k[255:224] }
  // 12 { ... , ctr+1      , ctr        }
  // 13 { ... , n[31:0]    , n[31:0]    }
  // ...
  // 15 { ... , n[95:64]   , n[95:64]   }
  st_ = st;
  return st_;
}



fn copy_state_x2(reg u256[4] st) -> reg u256[4]
{
  reg u256[4] k;
  k = st;
  return k;
}



fn copy_state_x4(reg u256[4] st) -> reg u256[4], reg u256[4]
{
  reg u256[4] k1 k2;
  k1 = st;
  k2 = st;
  k2[3] +8u32= g_p2;

  // k2                         k1
  // { sigma     , sigma      } { sigma     , sigma      }
  // { k[127:0]  , k[127:0]   } { k[127:0]  , k[127:0]   }
  // { k[255:128], k[255:128] } { k[255:128], k[255:128] }
  // { n , cnt+3 , n , cnt+2  } { n , cnt+1 , n , cnt    }
  return k1, k2;
}



fn copy_state_x8(stack u256[16] st) -> reg u256[16]
{
  reg u256[16] k;
  k = st;
  return k;
}



fn sum_states_x2(reg u256[4] k st) -> reg u256[4]
{
  inline int i;
  for i=0 to 4
  { k[i] +8u32= st[i]; }
  return k;
}



fn sum_states_x4(reg u256[4] k1 k2 st) -> reg u256[4], reg u256[4]
{
  inline int i;
  k1 = sum_states_x2(k1, st);
  k2 = sum_states_x2(k2, st);
  k2[3] +8u32= g_p2;
  return k1, k2;
}



fn sum_states_x8(reg u256[16] k, stack u256[16] st) -> reg u256[16]
{
  inline int i;
  for i=0 to 16
  { k[i] +8u32= st[i]; }
  return k;
}



fn perm_x2(reg u256[4] k) -> reg u256[4]
{
  reg u256[4] pk;
  pk[0] = #x86_VPERM2I128(k[0], k[1], 0x20);
  pk[1] = #x86_VPERM2I128(k[2], k[3], 0x20);
  pk[2] = #x86_VPERM2I128(k[0], k[1], 0x31);
  pk[3] = #x86_VPERM2I128(k[2], k[3], 0x31);
  return pk;
}



fn perm_x4(reg u256[4] k1 k2) -> reg u256[4], reg u256[4]
{
  reg u256[4] pk1 pk2;
  pk1 = perm_x2(k1);
  pk2 = perm_x2(k2);
  return pk1, pk2;
}



fn sub_rotate(reg u256[8] t) -> reg u256[8]
{
  inline int i;
  reg u256[8] x;

  x[0] = #x86_VPUNPCKL_4u64(t[0], t[1]);
  x[1] = #x86_VPUNPCKL_4u64(t[2], t[3]);
  x[2] = #x86_VPUNPCKH_4u64(t[0], t[1]);
  x[3] = #x86_VPUNPCKH_4u64(t[2], t[3]);

  x[4] = #x86_VPUNPCKL_4u64(t[4], t[5]);
  x[5] = #x86_VPUNPCKL_4u64(t[6], t[7]);
  x[6] = #x86_VPUNPCKH_4u64(t[4], t[5]);
  x[7] = #x86_VPUNPCKH_4u64(t[6], t[7]);

  for i=0 to 4
  {   t[i] = #x86_VPERM2I128(x[2*i+0], x[2*i+1], 0x20);
    t[i+4] = #x86_VPERM2I128(x[2*i+0], x[2*i+1], 0x31); }

  return t;
}



fn rotate(reg u256[8] x) -> reg u256[8]
{
  inline int i;
  reg u256[8] t;

  for i=0 to 4
  {   t[i] = #x86_VPUNPCKL_8u32(x[2*i+0], x[2*i+1]);
    t[i+4] = #x86_VPUNPCKH_8u32(x[2*i+0], x[2*i+1]); }

  t = sub_rotate(t);

  return t;
}



fn rotate_stack(stack u256[8] s) -> reg u256[8]
{
  inline int i;
  reg u256[8] t x;

  for i=0 to 4
  { x[i] = s[2*i+0]; }

  for i=0 to 4
  { t[  i] = #x86_VPUNPCKL_8u32(x[i], s[2*i+1]);
    t[4+i] = #x86_VPUNPCKH_8u32(x[i], s[2*i+1]); }

  t = sub_rotate(t);

  return t;
}



fn rotate_first_half_x8(reg u256[16] k) -> reg u256[8], stack u256[8]
{
  inline int i;
  stack u256[8] s_k8_15;
  reg   u256[8] k0_7;

  for i=0 to 8
  { s_k8_15[i] = k[8+i]; }

  for i=0 to 8
  { k0_7[i] = k[i]; }

  k0_7 = rotate(k0_7);

  return k0_7, s_k8_15;
}



fn rotate_second_half_x8(stack u256[8] s_k8_15) -> reg u256[8]
{
  inline int i;
  reg u256[8] k8_15;
  k8_15 = rotate_stack(s_k8_15);
  return k8_15;
}



fn interleave(stack u256[8] s, reg u256[8] k, inline int o) -> reg u256[8]
{
  inline int i;
  reg u256[8] sk;

  for i=0 to 4
  { sk[2*i+0] = s[o + i];
    sk[2*i+1] = k[o + i]; }

  return sk;
}



fn update_ptr(reg u64 output plain, reg u32 len, inline int n) -> reg u64, reg u64, reg u32
{
  output += n;
  plain += n;
  len -= n;
  return output, plain, len;
}



fn store(reg u64 output plain, reg u32 len, reg u256[2] k) -> reg u64, reg u64, reg u32, reg u256[2]
{
  k[0] ^= (u256)[plain +  0];
  k[1] ^= (u256)[plain + 32];

  (u256)[output +  0] = k[0];
  (u256)[output + 32] = k[1];

  output, plain, len = update_ptr(output, plain, len, 64);

  return output, plain, len, k;
}



fn store_last(reg u64 output plain, reg u32 len, reg u256[2] k)
{
  reg u256     r0;
  reg u128     r1;
  reg u64      r2 j;
  reg u8       r3;
  stack u8[16] s0; 

  r0 = k[0];

  if(len >= 32)
  {
    r0 ^= (u256)[plain + 0];
    (u256)[output + 0] = r0;

    output, plain, len = update_ptr(output, plain, len, 32);

    r0 = k[1];
  }

  r1 = #x86_VEXTRACTI128(r0, 0);

  if(len >= 16)
  {
    r1 ^= (u128)[plain + 0];
    (u128)[output + 0] = r1;

    output, plain, len = update_ptr(output, plain, len, 16);

    r1 = #x86_VEXTRACTI128(r0, 1);
  }

  s0[u128 0] = r1;

  j = 0;
  while(j < len)
  {
    r3 = (u8)[plain + j];
    r3 ^= s0[(int)j];
    (u8)[output + j] = r3;
    j += 1;
  }
}



fn store_x2(reg u64 output plain, reg u32 len, reg u256[4] k) -> reg u64, reg u64, reg u32, reg u256[4]
{
  inline int i;

  for i=0 to 4
  { k[i] ^= (u256)[plain + 32*i]; }

  for i=0 to 4
  { (u256)[output + 32*i] = k[i]; }

  output, plain, len = update_ptr(output, plain, len, 128);

  return output, plain, len, k;
}



fn store_x2_last(reg u64 output plain, reg u32 len, reg u256[4] k)
{
  reg u256[2] r;

  r[0] = k[0];
  r[1] = k[1];

  if(len >= 64)
  {
    output, plain, len, r = store(output, plain, len, r);
    r[0] = k[2];
    r[1] = k[3];
  }

  store_last(output, plain, len, r);
}



fn store_x4(reg u64 output plain, reg u32 len, reg u256[8] k) -> reg u64, reg u64, reg u32
{
  inline int i;

  for i=0 to 8
  { k[i] ^= (u256)[plain + 32*i]; }

  for i=0 to 8
  { (u256)[output + 32*i] = k[i]; }

  output, plain, len = update_ptr(output, plain, len, 256);

  return output, plain, len;
}



fn store_x4_last(reg u64 output plain, reg u32 len, reg u256[8] k)
{
  inline int i;
  reg u256[4] r;

  for i=0 to 4 { r[i] = k[i]; }

  if(len >= 128)
  {
    output, plain, len, r = store_x2(output, plain, len, r);
    for i=0 to 4 { r[i] = k[i+4]; }
  }

  store_x2_last(output, plain, len, r);
}



fn store_half_x8(reg u64 output plain, reg u32 len, reg u256[8] k, inline int o)
{
  inline int i;

  for i=0 to 8
  { k[i] ^= (u256)[plain + o + 64*i]; }
  for i=0 to 8
  { (u256)[output + o + 64*i] = k[i]; }
}



fn store_x8(reg u64 output plain, reg u32 len, reg u256[16] k) -> reg u64, reg u64, reg u32
{
  stack u256[8] s_k8_15;
  reg   u256[8] k0_7, k8_15;

  k0_7, s_k8_15 = rotate_first_half_x8(k);
  store_half_x8(output, plain, len, k0_7, i_0);
  k8_15 = rotate_second_half_x8(s_k8_15);
  store_half_x8(output, plain, len, k8_15, i_32);

  output, plain, len = update_ptr(output, plain, len, 512);

  return output, plain, len;
}



fn store_x8_last(reg u64 output plain, reg u32 len, reg u256[16] k)
{
  inline int i;
  stack u256[8] s_k0_7 s_k8_15;
  reg   u256[8] k0_7 k8_15 i0_7;

  k0_7, s_k8_15 = rotate_first_half_x8(k);
  s_k0_7 = k0_7;
  k8_15 = rotate_second_half_x8(s_k8_15);
  i0_7 = interleave(s_k0_7, k8_15, i_0);

  if(len >= 256)
  {
    output, plain, len = store_x4(output, plain, len, i0_7);
    i0_7 = interleave(s_k0_7, k8_15, i_4);
  }

  store_x4_last(output, plain, len, i0_7);
}



fn increment_counter_x8(stack u256[16] s) -> stack u256[16]
{
  reg u256 t;
  t = g_cnt_inc;
  t +8u32= s[12];
  s[12] = t;
  return s;
}



fn round_128(reg u256[4] k, reg u256 r16 r8) -> reg u256[4]
{
  reg u256 t;

  k[0] +8u32= k[1];
  k[3]     ^= k[0];
  k[3] = #x86_VPSHUFB_256(k[3], r16);

  k[2] +8u32= k[3];
  k[1]     ^= k[2];

     t = k[1] <<8u32 12;
  k[1] = k[1] >>8u32 20;
  k[1] ^= t;

  k[0] +8u32= k[1];
  k[3]     ^= k[0];
  k[3] = #x86_VPSHUFB_256(k[3], r8);

  k[2] +8u32= k[3];
  k[1] ^= k[2];

     t = k[1] <<8u32 7;
  k[1] = k[1] >>8u32 25;
  k[1] ^= t;

  return k;
}



fn round_256(reg u256[4] k1 k2, reg u256 r16 r8) -> reg u256[4], reg u256[4]
{
  reg u256 t1, t2;

  k1[0] +8u32= k1[1];
  k2[0] +8u32= k2[1];

  k1[3] ^= k1[0];
  k2[3] ^= k2[0];

  k1[3] = #x86_VPSHUFB_256(k1[3], r16);
  k2[3] = #x86_VPSHUFB_256(k2[3], r16);

  k1[2] +8u32= k1[3];
  k2[2] +8u32= k2[3];

                          k1[1] ^= k1[2];
     t1 = k1[1] <<8u32 12;
  k1[1] = k1[1] >>8u32 20;
                          k2[1] ^= k2[2];
  k1[1] ^= t1;

     t2 = k2[1] <<8u32 12;
  k2[1] = k2[1] >>8u32 20;
  k2[1] ^= t2;

  k1[0] +8u32= k1[1];
  k2[0] +8u32= k2[1];

  k1[3] ^= k1[0];
  k2[3] ^= k2[0];

  k1[3] = #x86_VPSHUFB_256(k1[3], r8);
  k2[3] = #x86_VPSHUFB_256(k2[3], r8);

  k1[2] +8u32= k1[3];
  k2[2] +8u32= k2[3];

                          k1[1] ^= k1[2];
     t1 = k1[1] <<8u32 7;
  k1[1] = k1[1] >>8u32 25;
                          k2[1] ^= k2[2];
  k1[1] ^= t1;

     t2 = k2[1] <<8u32 7;
  k2[1] = k2[1] >>8u32 25;
  k2[1] ^= t2;

  return k1, k2;
}



fn shuffle_state(reg u256[4] k) -> reg u256[4]
{
  k[1] = #x86_VPSHUFD_256(k[1], (4u2)[0,3,2,1]);
  k[2] = #x86_VPSHUFD_256(k[2], (4u2)[1,0,3,2]);
  k[3] = #x86_VPSHUFD_256(k[3], (4u2)[2,1,0,3]);
  return k;
}



fn reverse_shuffle_state(reg u256[4] k) -> reg u256[4]
{
  k[1] = #x86_VPSHUFD_256(k[1], (4u2)[2,1,0,3]);
  k[2] = #x86_VPSHUFD_256(k[2], (4u2)[1,0,3,2]);
  k[3] = #x86_VPSHUFD_256(k[3], (4u2)[0,3,2,1]);
  return k;
}



fn rounds_x2(reg u256[4] k) -> reg u256[4]
{
  reg u64 c;
  reg u256 r16 r8;

  r16 = g_r16;
  r8 = g_r8;
  c = 0;
  while(c < 10)
  {
    k = round_128(k, r16, r8);
    k = shuffle_state(k);
    k = round_128(k, r16, r8);
    k = reverse_shuffle_state(k);
    c += 1;
  }
  return k;
}



fn rounds_x4(reg u256[4] k1 k2) -> reg u256[4], reg u256[4]
{
  reg u64 c;
  reg u256 r16 r8;

  r16 = g_r16;
  r8 = g_r8;
  c = 0;
  while(c < 10)
  {
    k1, k2 = round_256(k1, k2, r16, r8);
        k1 = shuffle_state(k1);
        k2 = shuffle_state(k2);
    k1, k2 = round_256(k1, k2, r16, r8);
        k1 = reverse_shuffle_state(k1);
        k2 = reverse_shuffle_state(k2);
    c += 1;
  }
  return k1, k2;
}



fn round_512(reg u256[16] k, stack u256 s_r16 s_r8, inline int a0 b0 c0 d0 a1 b1 c1 d1) -> reg u256[16]
{
  reg u256 t;

  k[a0] +8u32= k[b0];
  k[a1] +8u32= k[b1];

  k[d0] ^= k[a0];
  k[d1] ^= k[a1];

  k[d0] = #x86_VPSHUFB_256(k[d0], s_r16);
  k[d1] = #x86_VPSHUFB_256(k[d1], s_r16);

  k[c0] +8u32= k[d0];
  k[c1] +8u32= k[d1];

                          k[b0] ^= k[c0];
      t = k[b0] <<8u32 12;
  k[b0] = k[b0] >>8u32 20;
                          k[b1] ^= k[c1];
  k[b0] ^= t;

      t = k[b1] <<8u32 12;
  k[b1] = k[b1] >>8u32 20;
  k[b1] ^= t;

  k[a0] +8u32= k[b0];
  k[a1] +8u32= k[b1];

  k[d0] ^= k[a0];
  k[d1] ^= k[a1];

  k[d0] = #x86_VPSHUFB_256(k[d0], s_r8);
  k[d1] = #x86_VPSHUFB_256(k[d1], s_r8);

  k[c0] +8u32= k[d0];
  k[c1] +8u32= k[d1];

                          k[b0] ^= k[c0];
      t = k[b0] <<8u32 7;
  k[b0] = k[b0] >>8u32 25;
                          k[b1] ^= k[c1];
  k[b0] ^= t;

      t = k[b1] <<8u32 7;
  k[b1] = k[b1] >>8u32 25;
  k[b1] ^= t;

  return k;
}



fn double_round_512(reg u256[16] k,
                    stack u256 k_d3 s_r16 s_r8,
                    inline int a0 b0 c0 d0
                               a1 b1 c1 d1
                               a2 b2 c2 d2
                               a3 b3 c3 d3
) -> reg u256[16], stack u256
{
  k = round_512(k, s_r16, s_r8, a0, b0, c0, d0, a1, b1, c1, d1);
  k[d3] = k_d3;
  k_d3 = k[d1];
  k = round_512(k, s_r16, s_r8, a2, b2, c2, d2, a3, b3, c3, d3);
  return k, k_d3;
}



fn rounds_x8(reg u256[16] k, stack u256 s_r16 s_r8) -> reg u256[16]
{
  reg u64 c;
  stack u256 k14, k15;

  k15 = k[15];

  c = 0;
  while(c < 10)
  {
    k, k14 = double_round_512(k, k15, s_r16, s_r8,
                                      i_0, i_4, i_8,  i_12,
                                      i_2, i_6, i_10, i_14,
                                      i_1, i_5, i_9,  i_13,
                                      i_3, i_7, i_11, i_15);

    k, k15 = double_round_512(k, k14, s_r16, s_r8,
                                      i_1, i_6, i_11, i_12,
                                      i_0, i_5, i_10, i_15,
                                      i_2, i_7, i_8,  i_13,
                                      i_3, i_4, i_9,  i_14);
    c += 1;
  }

  k[15] = k15;

  return k;
}



fn chacha20_more_than_256(reg u64 output plain, reg u32 len, reg u64 key nonce, reg u32 counter)
{
  stack u256[16] st;
  reg   u256[16]  k;
  stack u256 s_r16 s_r8;

  s_r16, s_r8 = load_shufb_cmd();

  st = init_x8(key, nonce, counter);

  while(len >= 512)
  {
    k = copy_state_x8(st);
    k = rounds_x8(k, s_r16, s_r8);
    k = sum_states_x8(k, st);
    output, plain, len = store_x8(output, plain, len, k);
    st = increment_counter_x8(st);
  }

  if(len > 0)
  {
    k = copy_state_x8(st);
    k = rounds_x8(k, s_r16, s_r8);
    k = sum_states_x8(k, st);
    store_x8_last(output, plain, len, k);
  }
}



fn chacha20_less_than_257(reg u64 output plain, reg u32 len, reg u64 key nonce, reg u32 counter)
{
  reg u256[4] st k1 k2;

  st = init_x2(key, nonce, counter); 

  if(len > 128)
  {
    k1, k2 = copy_state_x4(st);
    k1, k2 = rounds_x4(k1, k2);
    k1, k2 = sum_states_x4(k1, k2, st);
    k1, k2 = perm_x4(k1, k2);
    output, plain, len, k1 = store_x2(output, plain, len, k1);
                             store_x2_last(output, plain, len, k2);
  }
  else
  {
    k1 = copy_state_x2(st);
    k1 = rounds_x2(k1);
    k1 = sum_states_x2(k1, st);
    k1 = perm_x2(k1);
         store_x2_last(output, plain, len, k1);
  }
}



export fn chacha20_avx2(reg u64 output plain, reg u32 len, reg u64 key nonce, reg u32 counter)
{
  if(len < 257)
  { chacha20_less_than_257(output, plain, len, key, nonce, counter); }
  else
  { chacha20_more_than_256(output, plain, len, key, nonce, counter); }
}
