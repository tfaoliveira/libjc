param int A_jagged_0 =    0/8;
param int A_jagged_1 =   32/8;
param int A_jagged_2 =   40/8;
param int A_jagged_3 =   48/8;
param int A_jagged_4 =   56/8;
param int A_jagged_5 =   80/8;
param int A_jagged_6 =  192/8;
param int A_jagged_7 =  104/8;
param int A_jagged_8 =  144/8;
param int A_jagged_9 =  184/8;
param int A_jagged_10 =  64/8;
param int A_jagged_11 = 128/8;
param int A_jagged_12 = 200/8;
param int A_jagged_13 = 176/8;
param int A_jagged_14 = 120/8;
param int A_jagged_15 =  88/8;
param int A_jagged_16 =  96/8;
param int A_jagged_17 = 168/8;
param int A_jagged_18 = 208/8;
param int A_jagged_19 = 152/8;
param int A_jagged_20 =  72/8;
param int A_jagged_21 = 160/8;
param int A_jagged_22 = 136/8;
param int A_jagged_23 = 112/8;
param int A_jagged_24 = 216/8;

inline fn __keccak_f(
  reg u256 A00 A01 A20 A31
           A21 A41 A11,
  reg u64 _rhotates_left
          _rhotates_right
          _iotas
) -> reg u256, reg u256, reg u256, reg u256,
     reg u256, reg u256, reg u256
{
  reg u32 i; // why 32-bit?
  reg u256 C00, C14; 
  reg u256 D00, D14;
  reg u256[9] T;
  reg bool zf;
  reg u64 rhotates_left rhotates_right iotas; // these should all be 32-bit

  rhotates_left = _rhotates_left + 96;
  rhotates_right = _rhotates_right + 96;
  iotas = _iotas;

  i = 24;
  while
  {
	  /* ######################################### Theta */
	  C00  = #x86_VPSHUFD_256(A20, 0x4e /*0b01001110 */);
	  C14  = A41 ^4u64 A31 ;
	  T[2] = A21 ^4u64 A11 ;
	  C14  = C14 ^4u64 A01 ;
	  C14  = C14 ^4u64 T[2];

	  T[4] = #x86_VPERMQ(C14, 0x93 /* 0b10010011 */);
	  C00 = C00 ^4u64 A20;
	  T[0] = #x86_VPERMQ(C00, 0x4e /* 0b01001110 */);


      T[1] = C14 >>4u64 63; 
      T[2] = C14 +4u64 C14;
	  T[1] = T[1] |4u64 T[2];

	  D14 = #x86_VPERMQ(T[1], 0x39 /*0b00111001 */);
	  D00 = T[1] ^4u64 T[4];
	  D00 = #x86_VPERMQ(D00, 0x00 /*0b00000000 */);

	  C00 = C00 ^4u64 A00;
	  C00 = C00 ^4u64 T[0];

      T[0] = C00 >>4u64 63; 
      T[1] = C00 +4u64 C00;
	  T[1] = T[1] |4u64 T[0];

	  A20 = A20 ^4u64 D00;
	  A00 = A00 ^4u64 D00;

      D14  = #x86_VPBLENDD_256(D14,T[1], (4u2)[3,0,0,0] /* 0b11000000 */);
      T[4] = #x86_VPBLENDD_256(T[4],C00, (4u2)[0,0,0,3] /* 0b00000011 */);
	  D14 = D14 ^4u64 T[4];

	  /* ######################################### Rho + Pi + pre-Chi shuffle */

	  T[3] = #x86_VPSLLV_4u64(A20,(u256)[rhotates_left + 0*32-96]); //vpsllvq 0*32-96(%r8),$A20,@T[3]
	  A20  = #x86_VPSRLV_4u64(A20,(u256)[rhotates_right+ 0*32-96]); //vpsrlvq	0*32-96(%r9),$A20,$A20
	  A20 = A20 |4u64 T[3];

	  A31 = A31 ^4u64 D14;
	  T[4] = #x86_VPSLLV_4u64(A31,(u256)[rhotates_left + 2*32-96]); //vpsllvq 2*32-96(%r8),$A31,@T[4]
	  A31  = #x86_VPSRLV_4u64(A31,(u256)[rhotates_right+ 2*32-96]); //vpsllvq 2*32-96(%r9),$A31,$A31
	  A31 = A31 |4u64 T[4];

	  A21 = A21 ^4u64 D14;
	  T[5] = #x86_VPSLLV_4u64(A21,(u256)[rhotates_left + 3*32-96]); //vpsllvq 3*32-96(%r8),$A21,@T[5]
	  A21  = #x86_VPSRLV_4u64(A21,(u256)[rhotates_right+ 3*32-96]); //vpsrlvq 3*32-96(%r9),$A21,$A21
	  A21 = A21 |4u64 T[5];
			

	  A41 = A41 ^4u64 D14;
	  T[6] = #x86_VPSLLV_4u64(A41,(u256)[rhotates_left + 4*32-96]); //vpsllvq 4*32-96(%r8),$A41,@T[6]
	  A41  = #x86_VPSRLV_4u64(A41,(u256)[rhotates_right+ 4*32-96]); //vpsrlvq 4*32-96(%r9),$A41,$A41
	  A41 = A41 |4u64 T[6];

	  A11 = A11 ^4u64 D14;
	  T[3] = #x86_VPERMQ(A20, 0x8d /*0b10001101 */);
	  T[4] = #x86_VPERMQ(A31, 0x8d /*0b10001101 */);
	  T[7] = #x86_VPSLLV_4u64(A11,(u256)[rhotates_left + 5*32-96]); //vpsllvq 5*32-96(%r8),$A11,@T[7]
	  T[1] = #x86_VPSRLV_4u64(A11,(u256)[rhotates_right+ 5*32-96]); //vpsrlvq 5*32-96(%r9),$A11,@T[1]
	  T[1] = T[1] |4u64 T[7];

	  A01 = A01 ^4u64 D14;
	  T[5] = #x86_VPERMQ(A21, 0x1b /*0b00011011 */);
	  T[6] = #x86_VPERMQ(A41, 0x72 /*0b01110010 */);
	  T[8] = #x86_VPSLLV_4u64(A01,(u256)[rhotates_left + 1*32-96]); //vpsllvq 1*32-96(%r8),$A01,@T[8]
	  T[2] = #x86_VPSRLV_4u64(A01,(u256)[rhotates_right+ 1*32-96]); //vpsrlvq 1*32-96(%r9),$A01,@T[2]
	  T[2] = T[2] |4u64 T[8];


	  /* ######################################### Chi */
      T[7] = #x86_VPSRLDQ_256(T[1], 8); // >>2u128 vpsrldq
	  T[0] = !T[1] & T[7]; // #x86_VPANDN_256(T[1],T[7]);  

      A31  = #x86_VPBLENDD_256(T[2],T[6], (4u2)[0,0,3,0] );
      T[8] = #x86_VPBLENDD_256(T[4],T[2], (4u2)[0,0,3,0] );
      A41  = #x86_VPBLENDD_256(T[3],T[4], (4u2)[0,0,3,0] );
      T[7] = #x86_VPBLENDD_256(T[2],T[3], (4u2)[0,0,3,0] );

      A31  = #x86_VPBLENDD_256(A31, T[4], (4u2)[0,3,0,0] );
      T[8] = #x86_VPBLENDD_256(T[8],T[5], (4u2)[0,3,0,0] );
      A41  = #x86_VPBLENDD_256(A41, T[2], (4u2)[0,3,0,0] );
      T[7] = #x86_VPBLENDD_256(T[7],T[6], (4u2)[0,3,0,0] );

      A31  = #x86_VPBLENDD_256(A31, T[5], (4u2)[3,0,0,0] );
      T[8] = #x86_VPBLENDD_256(T[8],T[6], (4u2)[3,0,0,0] );
      A41  = #x86_VPBLENDD_256(A41, T[6], (4u2)[3,0,0,0] );
      T[7] = #x86_VPBLENDD_256(T[7],T[4], (4u2)[3,0,0,0] );

	  A31 = #x86_VPANDN_256(A31,T[8]);
	  A41 = #x86_VPANDN_256(A41,T[7]);

      A11  = #x86_VPBLENDD_256(T[5],T[2], (4u2)[0,0,3,0] );
      T[8] = #x86_VPBLENDD_256(T[3],T[5], (4u2)[0,0,3,0] ); 
	  A31 = A31 ^4u64 T[3];

      A11  = #x86_VPBLENDD_256(A11,T[3],  (4u2)[0,3,0,0] );
      T[8] = #x86_VPBLENDD_256(T[8],T[4], (4u2)[0,3,0,0] );
	  A41 = A41 ^4u64 T[5];

      A11  = #x86_VPBLENDD_256(A11,T[4],  (4u2)[3,0,0,0] );
      T[8] = #x86_VPBLENDD_256(T[8],T[2], (4u2)[3,0,0,0] );
	  A11 = #x86_VPANDN_256(A11,T[8]);
	  A11 = A11 ^4u64 T[6];

	  A21 = #x86_VPERMQ(T[1], 0x1e /*0b00011110 */);
      T[8] = #x86_VPBLENDD_256(A21,A00, (4u2)[0,3,0,0] );
	  A01 = #x86_VPERMQ(T[1], 0x39 /*0b00111001 */);
      A01 = #x86_VPBLENDD_256(A01,A00, (4u2)[3,0,0,0] );
	  A01 = #x86_VPANDN_256(A01,T[8]);

      A20  = #x86_VPBLENDD_256(T[4],T[5], (4u2)[0,0,3,0] );
      T[7] = #x86_VPBLENDD_256(T[6],T[4], (4u2)[0,0,3,0] );
      A20  = #x86_VPBLENDD_256(A20, T[6], (4u2)[0,3,0,0] );
      T[7] = #x86_VPBLENDD_256(T[7],T[3], (4u2)[0,3,0,0] );
      A20  = #x86_VPBLENDD_256(A20, T[3], (4u2)[3,0,0,0] );
      T[7] = #x86_VPBLENDD_256(T[7],T[5], (4u2)[3,0,0,0] );

	  A20 = #x86_VPANDN_256(A20,T[7]);
	  A20 = A20 ^4u64 T[2];

	  T[0] = #x86_VPERMQ(T[0], 0x00/*0b00000000 */);
	  A31  = #x86_VPERMQ(A31, 0x1b/*0b00011011 */);
	  A41  = #x86_VPERMQ(A41, 0x8d/*0b10001101 */);
	  A11  = #x86_VPERMQ(A11, 0x72/*0b01110010 */);

      A21  = #x86_VPBLENDD_256(T[6],T[3], (4u2)[0,0,3,0] );
      T[7] = #x86_VPBLENDD_256(T[5],T[6], (4u2)[0,0,3,0] );
      A21  = #x86_VPBLENDD_256(A21, T[5], (4u2)[0,3,0,0] );
      T[7] = #x86_VPBLENDD_256(T[7],T[2], (4u2)[0,3,0,0] );
      A21  = #x86_VPBLENDD_256(A21, T[2], (4u2)[3,0,0,0] );
      T[7] = #x86_VPBLENDD_256(T[7],T[3], (4u2)[3,0,0,0] );

	  A21 = #x86_VPANDN_256(A21,T[7]);

	  A00 = A00 ^4u64 T[0];
	  A01 = A01 ^4u64 T[1];
	  A21 = A21 ^4u64 T[4];

      /*	######################################### Iota */
	  A00 = A00 ^4u64 (u256)[iotas + 0];
	  iotas = iotas + 32;

    (_,_,_,zf,i) = #x86_DEC_32(i); // dec = decl?
   } (!zf)

   return A00, A01, A20, A31, A21, A41, A11;
}

fn keccak_f(
  reg u64 r _rhotates_left _rhotates_right _iotas
) -> reg u64
{
  reg u256 A00, A01, A20, A31,
           A21, A41, A11;

  A00 = (u256)[r +   0];
  A01 = (u256)[r +  32];
  A20 = (u256)[r +  64];
  A31 = (u256)[r +  96];
  A21 = (u256)[r + 128];
  A41 = (u256)[r + 160];
  A11 = (u256)[r + 192];

	A00, A01, A20, A31, A21, A41, A11 = __keccak_f(A00, A01, A20, A31,
                                                 A21, A41, A11,
		                                             _rhotates_left,
                                                 _rhotates_right,
                                                 _iotas);

  (u256)[r +   0] = A00;
  (u256)[r +  32] = A01;
  (u256)[r +  64] = A20;
  (u256)[r +  96] = A31;
  (u256)[r + 128] = A21;
  (u256)[r + 160] = A41;
  (u256)[r + 192] = A11;

  // easy to support state in stack if the types are right
  return r;
}
