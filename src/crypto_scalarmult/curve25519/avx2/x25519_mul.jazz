#ifndef X25519_AVX2_MUL
#define X25519_AVX2_MUL

#include "crypto_scalarmult/curve25519/avx2/x25519_globals.jazz"
#include "crypto_scalarmult/curve25519/avx2/x25519_macros.jazz"

inline fn __rrx_mul_rss(stack u256[5] a, stack u256[5] b, stack u256 sh_0 sh_1) -> reg u256[5]
{
  inline int i;
	reg u256 x ai aj;
	reg u256[5] br dr er c k t;
	stack u256[5] d e;

  br = b;

  for i=4 downto -1
  { er[i] = MUL(br[i], g_times_19);
    //dr[i] = ALIGNX(br[i],er[i]); //TODO
    ALIGNR(dr[i],br[i],er[i]);
    e[i]  = er[i]; 
    d[i]  = dr[i]; }

	ai = SHUF32(a[0],0x44);  	      aj = SHUF32(a[1],0x44);
	c[0] = MUL(ai,b[0]);       		  k[0] = MUL(aj,d[4]);
	c[1] = MUL(ai,b[1]);       		  k[1] = MUL(aj,b[0]);
	c[2] = MUL(ai,b[2]);       		  k[2] = MUL(aj,b[1]);
	c[3] = MUL(ai,b[3]);       		  k[3] = MUL(aj,b[2]);
	c[4] = MUL(ai,b[4]);       		  k[4] = MUL(aj,b[3]);

	ai = SHUF32(a[2], 0x44);        aj = SHUF32(a[3], 0x44);
	ADD_MUL(c[0],ai,d[3]);          ADD_MUL(k[0],aj,d[2]);
	ADD_MUL(c[1],ai,d[4]);          ADD_MUL(k[1],aj,d[3]);
	ADD_MUL(c[2],ai,b[0]);          ADD_MUL(k[2],aj,d[4]);
	ADD_MUL(c[3],ai,b[1]);          ADD_MUL(k[3],aj,b[0]);
	ADD_MUL(c[4],ai,b[2]);          ADD_MUL(k[4],aj,b[1]);

	ai = SHUF32(a[4],0x44);         aj = SHUF32(a[0],0xEE);
	ADD_MUL(c[0],ai,d[1]);          ADD_MUL(k[0],aj,d[0]);
	ADD_MUL(c[1],ai,d[2]);          ADD_MUL(k[1],aj,d[1]);
	ADD_MUL(c[2],ai,d[3]);          ADD_MUL(k[2],aj,d[2]);
	ADD_MUL(c[3],ai,d[4]);          ADD_MUL(k[3],aj,d[3]);
	ADD_MUL(c[4],ai,b[0]);          ADD_MUL(k[4],aj,d[4]);

	ai = SHUF32(a[1],0xEE);         aj = SHUF32(a[2],0xEE);
	ADD_MUL(c[0],ai,e[4]);          ADD_MUL(k[0],aj,e[3]);
	ADD_MUL(c[1],ai,d[0]);          ADD_MUL(k[1],aj,e[4]);
	ADD_MUL(c[2],ai,d[1]);          ADD_MUL(k[2],aj,d[0]);
	ADD_MUL(c[3],ai,d[2]);          ADD_MUL(k[3],aj,d[1]);
	ADD_MUL(c[4],ai,d[3]);          ADD_MUL(k[4],aj,d[2]);

	ai = SHUF32(a[3],0xEE);         aj = SHUF32(a[4],0xEE);
	ADD_MUL(c[0],ai,e[2]);          ADD_MUL(k[0],aj,e[1]);
	ADD_MUL(c[1],ai,e[3]);          ADD_MUL(k[1],aj,e[2]);
	ADD_MUL(c[2],ai,e[4]);          ADD_MUL(k[2],aj,e[3]);
	ADD_MUL(c[3],ai,d[0]);          ADD_MUL(k[3],aj,e[4]);
	ADD_MUL(c[4],ai,d[1]);          ADD_MUL(k[4],aj,d[0]);

	t[0] = SHLV(k[0],sh_0); t[1] = SHLV(k[1],sh_1);
	c[0] +4u64= t[0];         c[1] +4u64= t[1];
	t[2] = SHLV(k[2],sh_0); t[3] = SHLV(k[3],sh_1);
	c[2] +4u64= t[2];         c[3] +4u64= t[3];
	t[4] = SHLV(k[4],sh_0);
	c[4] +4u64= t[4];

  return c;
}

inline fn __rrx_mul_rmm(reg u64 a b) -> reg u256[5]
{
  inline int i;
	reg u256 x ai aj;
	reg u256[5] br dr er c k t;
	stack u256[5] d e;

  for i=0 to 5
  { br[i] = (u256)[b + 32*i]; }

  for i=4 downto -1
  { er[i] = MUL(br[i], g_times_19);
    //dr[i] = ALIGNX(br[i],er[i]); //TODO
    ALIGNR(dr[i],br[i],er[i]);
    e[i]  = er[i];
    d[i]  = dr[i];
  }

	ai = SHUF32_M(a,0,0x44);  	    aj = SHUF32_M(a,1,0x44);
	c[0] = MUL_M(ai,b,0);       		k[0] = MUL(aj,d[4]);
	c[1] = MUL_M(ai,b,1);       		k[1] = MUL_M(aj,b,0);
	c[2] = MUL_M(ai,b,2);       		k[2] = MUL_M(aj,b,1);
	c[3] = MUL_M(ai,b,3);       		k[3] = MUL_M(aj,b,2);
	c[4] = MUL_M(ai,b,4);       		k[4] = MUL_M(aj,b,3);

	ai = SHUF32_M(a,2,0x44);        aj = SHUF32_M(a,3,0x44);
	ADD_MUL(  c[0],ai,d[3]);        ADD_MUL(  k[0],aj,d[2]);
	ADD_MUL(  c[1],ai,d[4]);        ADD_MUL(  k[1],aj,d[3]);
	ADD_MUL_M(c[2],ai,b,0 );        ADD_MUL(  k[2],aj,d[4]);
	ADD_MUL_M(c[3],ai,b,1 );        ADD_MUL_M(k[3],aj,b,0 );
	ADD_MUL_M(c[4],ai,b,2 );        ADD_MUL_M(k[4],aj,b,1 );

	ai = SHUF32_M(a,4,0x44);        aj = SHUF32_M(a,0,0xEE);
	ADD_MUL(  c[0],ai,d[1]);        ADD_MUL(k[0],aj,d[0]);
	ADD_MUL(  c[1],ai,d[2]);        ADD_MUL(k[1],aj,d[1]);
	ADD_MUL(  c[2],ai,d[3]);        ADD_MUL(k[2],aj,d[2]);
	ADD_MUL(  c[3],ai,d[4]);        ADD_MUL(k[3],aj,d[3]);
	ADD_MUL_M(c[4],ai,b,0 );        ADD_MUL(k[4],aj,d[4]);

	ai = SHUF32_M(a,1,0xEE);        aj = SHUF32_M(a,2,0xEE);
	ADD_MUL(c[0],ai,e[4]);          ADD_MUL(k[0],aj,e[3]);
	ADD_MUL(c[1],ai,d[0]);          ADD_MUL(k[1],aj,e[4]);
	ADD_MUL(c[2],ai,d[1]);          ADD_MUL(k[2],aj,d[0]);
	ADD_MUL(c[3],ai,d[2]);          ADD_MUL(k[3],aj,d[1]);
	ADD_MUL(c[4],ai,d[3]);          ADD_MUL(k[4],aj,d[2]);

	ai = SHUF32_M(a,3,0xEE);        aj = SHUF32_M(a,4,0xEE);
	ADD_MUL(c[0],ai,e[2]);          ADD_MUL(k[0],aj,e[1]);
	ADD_MUL(c[1],ai,e[3]);          ADD_MUL(k[1],aj,e[2]);
	ADD_MUL(c[2],ai,e[4]);          ADD_MUL(k[2],aj,e[3]);
	ADD_MUL(c[3],ai,d[0]);          ADD_MUL(k[3],aj,e[4]);
	ADD_MUL(c[4],ai,d[1]);          ADD_MUL(k[4],aj,d[0]);

	t[0] = SHLV(k[0],g_sh_0);
	t[1] = SHLV(k[1],g_sh_1);
	t[2] = SHLV(k[2],g_sh_0);
	t[3] = SHLV(k[3],g_sh_1);
	t[4] = SHLV(k[4],g_sh_0);

	c[0] +4u64= t[0];
	c[1] +4u64= t[1];
	c[2] +4u64= t[2];
	c[3] +4u64= t[3];
	c[4] +4u64= t[4];

  return c;
}

#ifdef EXPORT

export fn rrx_mul_rss(reg u64 cp ap bp)
{
  inline int i;
  reg u256 v0 v1 av bv;
  reg u256[5] c;
  stack u256[5] a b;
  stack u256 sh_0 sh_1;

  v0 = g_sh_0; v1 = g_sh_1;
  sh_0 = v0;   sh_1 = v1;

  for i=0 to 5
  { av = (u256)[ap + 32*i];
    bv = (u256)[bp + 32*i];
    a[i] = av;
    b[i] = bv;
  }
  c = __rrx_mul_rss(a, b, sh_0, sh_1);

  for i=0 to 5
  { (u256)[cp + 32*i] = c[i]; }
}

export fn rrx_mul_rmm(reg u64 cp ap bp)
{
  inline int i;
  reg u256[5] cr;
  cr = __rrx_mul_rmm(ap, bp);
  for i=0 to 5
  { (u256)[cp + 32*i] = cr[i]; }
}

#endif

#endif

