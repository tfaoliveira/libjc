#ifndef X25519_AVX2_LADDER
#define X25519_AVX2_LADDER

#include "crypto_scalarmult/curve25519/avx2/x25519_globals.jazz"
#include "crypto_scalarmult/curve25519/avx2/x25519_macros.jazz"
#include "crypto_scalarmult/curve25519/avx2/x25519_mul.jazz"
#include "crypto_scalarmult/curve25519/avx2/x25519_sqr.jazz"
#include "crypto_scalarmult/curve25519/avx2/x25519_addsub.jazz"
#include "crypto_scalarmult/curve25519/avx2/x25519_inter.jazz"
#include "crypto_scalarmult/curve25519/avx2/x25519_compress.jazz"
#include "crypto_scalarmult/curve25519/avx2/x25519_swap.jazz"
#include "crypto_scalarmult/curve25519/avx2/x25519_ser.jazz"

// @pre: key[3] needs to be shifted left by 1
inline fn __rrx_step_ladder(stack u64[4] key, reg u256[5] X2X3 Z2Z3) -> reg u256[5], reg u256[5]
{
  inline int s;
  reg bool cf;
  reg u64 i j k bit swap nswap prev;
  reg u256 v0 v1 zero a24;
  reg u256[5] X1 t0 t1 t2 t3 _2P;
  stack u256[5] X2X3s Z2Z3s X1s t0s t1s t2s;
  stack u256 sh_0 sh_1;

  #if 0
  reg u256 mask maskbit0 maskbit1;
  reg u32 u32_swap;
  stack u32 u32_swaps;
  #endif

  v0 = g_sh_0; v1 = g_sh_1;
  sh_0 = v0;   sh_1 = v1;

  X1s = X2X3;

  prev = #set0();
  i = 3;
  j = 63;
  while
  { 
    k = key[(int) i];
    while
    {
      bit = #set0();
      _, cf, _, _, _, k = #SHL(k, 1);
      _, bit += 0 + cf;
      swap = bit;
      swap ^= prev;
      prev = bit;

      /* X2X3: [A|C] = [X2|X3] + [Z2|Z3]  */
      /* Z2Z3: [B|D] = [X2|X3] - [Z2|Z3]  */
      X2X3s, Z2Z3 = __rrx_addsub_srrr(X2X3, Z2Z3);

      /* Z2Z3: [D|B] = Permute([B|D])     */
      for s=0 to 5
      { Z2Z3[s] = PERM64(Z2Z3[s], 0x4E);
        Z2Z3s[s] = Z2Z3[s]; }

      /* t0:   [DA|CB] = [A|C] * [D|B]    */
      t0 = __rrx_mul_rss(X2X3s, Z2Z3s, sh_0, sh_1);
      t0 = __rrx_compress_r(t0);

      /* t1:   [CB|DA] = Perm([DA|CB])    */
      for s=0 to 5
      { t1[s] = PERM64(t0[s], 0x4E); 
        t0s[s] = t0[s]; }
      t1s = t1;

      #if 0 
      /*TODO: NEED VPERMD */
      /* SWAP_METHOD == PERMUTATION       */
      /* Using vector permutation instructions  */
      maskbit0 = g_swap_perm;
      u32_swap = (32u) swap;
      u32_swap <<= 2;
      u32_swaps = u32_swap;
      mask = #VPBROADCAST_8u32(u32_swaps);
      maskbit0 ^= mask;
      maskbit1  = PERM64(maskbit0, 0x4E);
      for s=0 to 5
      { X2X3[s] = PERMV128(X2X3s[s], maskbit0);
        Z2Z3[s] = PERMV128(Z2Z3s[s], maskbit1); }
      #else

      /* SWAP_METHOD == CMOV              */
      /* Using 64-bit CMOV instruction    */
      X2X3s = __rrx_cmov1_ss(swap,X2X3s,0,4);
      nswap = 1;
      nswap -= swap;
      Z2Z3s = __rrx_cmov1_ss(nswap,Z2Z3s,0,4);
      #endif

      /* t1:  [_|t1] = [CB|DA] + [DA|CB]  */
      /* t0:  [_|t0] = [CB|DA] - [DA|CB]  */
      t1, t0 = __rrx_addsub_rrss(t1s, t0s);

      /* t0:  [  A | t0 ] = BLEND( [ A | C ] , [ __ | t0 ] ) */
      /* t1:  [  B | t1 ] = BLEND( [ B | D ] , [ __ | t1 ] ) */
      for s=0 to 5
      { t0[s] = BLEND32(t0[s], X2X3s[s], 0x0F);
        t1[s] = BLEND32(t1[s], Z2Z3s[s], 0x0F);
      }
      t0s = t0;
      t1s = t1;

      /* t0:  [ AA | t0 ] = [  A | t0 ]^2 */
      t0s = __rrx_sqr_ss(t0s, sh_0, sh_1);

      /* t1:  [ BB | t1 ] = [  B | t1 ]^2 */
      t1  = __rrx_sqr_rs(t1s, sh_0, sh_1);
      t0  = t0s;
      /*  Reducing coefficient size       */
      t0, t1 = __rrx_compress2_rr(t0, t1);
      t1s = t1;

      /* t2:  [ AA |  1 ] = BLEND( [ AA | t0 ] , [ 0 | 0 ] ) */
      zero = #set0_256();
      for s=0 to 5
      { t2[s] = BLEND32(t0[s], zero, 0xF0); }
      t2[0] |= g_3one;
      t0s = t0;
      t2s = t2;

      /* X2X3: [ X2 | X3 ] = [ AA |  1 ] * [ BB | t1 ]  */
      X2X3 = __rrx_mul_rss(t2s, t1s, sh_0, sh_1);
      /*  Reducing coefficient size       */
      X2X3 = __rrx_compress_r(X2X3);
      X2X3s = X2X3;

      /* t2: [  E | __ ] = [ AA | t0 ] - [ BB | t1 ]  */
      /* t1: [a24E| __ ] =         a24 * [  E | __ ]  */
      /* t1: [  F | __ ] = [a24E| __ ] + [ AA | t0 ]  */
      /* t2: [  E | t0 ] = BLEND( [ E | _ ] , [ AA | t0 ] ) */
      /* t1: [  F | X1 ] = BLEND( [ F | _ ] , [ __ | X1 ] ) */
      t1  = t1s;
      t2  = t2s;
      a24 = g_a24;
      for s=0 to 5
      {
        // t2[s] = t0[s] + (_2P[s] - t1[s])
        _2P[s] = CONST_2P_2P_H0H5[s];
        _2P[s] -4u64= t1[s];
        t2[s]  = _2P[s] +4u64 t0s[s];

        // t1[s] = ADD(MUL(t2[s], a24), t0[s]);
        t3[s] = MUL(t2[s], a24);
        t1[s] = t3[s] +4u64 t0s[s];

        t2[s] = BLEND32(t2[s], t0s[s], 0xF0);
        t1[s] = BLEND32(t1[s], X1s[s], 0xF0);
      }
      t2s = t2;

      /*  Reducing coefficient size       */
      t1 = __rrx_compressfast_r(t1);
      t1s = t1;

      /* Z2Z3: [ Z2 | Z3 ] = [  E | t0 ] * [  F | X1 ]  */
      Z2Z3 = __rrx_mul_rss(t2s, t1s, sh_0, sh_1);
      /*  Reducing coefficient size       */
      Z2Z3 = __rrx_compress_r(Z2Z3);
      X2X3 = X2X3s;

      j -= 1;
    }(j > 0)

    j = 64;
    i -= 1;
  }(i >=s 0)

  return X2X3, Z2Z3;
}

#ifdef EXPORT

export fn rrx_step_ladder(reg u64 kp xp zp)
{
  inline int i;
  reg u64[4] keyr;
  stack u64[4] key;
  reg u256[5] X2X3 Z2Z3;

  for i=0 to 4
  { keyr[i] = [kp + 8*i]; }
  keyr[3] <<= 1;
  key = keyr;

  for i=0 to 5
  { X2X3[i] = (u256)[xp + 32*i];
    Z2Z3[i] = (u256)[zp + 32*i]; }

  X2X3,Z2Z3 = __rrx_step_ladder(key,X2X3,Z2Z3);

  for i=0 to 5
  { (u256)[xp + 32*i] = X2X3[i];
    (u256)[zp + 32*i] = Z2Z3[i]; }
}

#endif

#endif

