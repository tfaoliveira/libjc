#ifndef X25519_MUL
#define X25519_MUL
#include "x25519_reduce.japp"

// note: _l stands for the lower 64 bits of multiplication, _h for higher
//
// 
// h = f * g mod 2**255-19
//
// with
//
// f = f0 * 2**0 + f1 * 2**64 + f2 * 2**128 + f3 * 2**192
// g = g0 * 2**0 + g1 * 2**64 + g2 * 2**128 + g3 * 2**192
//
// with each of the limbs being >= 0 and < 2**64
//
// h = f * g
//
// <=>
//
// h = 2**0   * ( f0*g0 )                         +
//     2**64  * ( f0*g1 + f1*g0 )                 +
//     2**128 * ( f0*g2 + f1*g1 + f2*g0 )         +
//     2**192 * ( f0*g3 + f1*g2 + f2*g1 + f3*g0 ) + 
//     2**256 * ( f1*g3 + f2*g2 + f3*g1 )         +
//     2**320 * ( f2*g3 + f3*g2 )                 +
//     2**384 * ( f3*g3 )
//
// <=>
//
// h = 2**0   * ( f0*g0 + ( f1*g3 +   f2*g2 +   f3*g1 ) * 38 ) +
//     2**64  * ( f0*g1 +   f1*g0 + ( f2*g3 +   f3*g2 ) * 38 ) +
//     2**128 * ( f0*g2 +   f1*g1 +   f2*g0 + ( f3*g3 ) * 38 ) +
//     2**192 * ( f0*g3 +   f1*g2 +   f2*g1 +   f3*g0 )
//
// <=>
//
// h = 2**0   * ( f0*g0_l + ( f1*g3_l +   f2*g2_l +   f3*g1_l ) * 38 ) +
//     2**64  * ( f0*g0_h + ( f1*g3_h +   f2*g2_h +   f3*g1_h ) * 38 ) +
//
//     2**64  * ( f0*g1_l +   f1*g0_l + ( f2*g3_l +   f3*g2_l ) * 38 ) +
//     2**128 * ( f0*g1_h +   f1*g0_h + ( f2*g3_h +   f3*g2_h ) * 38 ) +
//
//     2**128 * ( f0*g2_l +   f1*g1_l +   f2*g0_l + ( f3*g3_l ) * 38 ) +
//     2**192 * ( f0*g2_h +   f1*g1_h +   f2*g0_h + ( f3*g3_h ) * 38 ) +
//
//     2**192 * ( f0*g3_l +   f1*g2_l +   f2*g1_l +   f3*g0_l )        +
//     2**256 * ( f0*g3_h +   f1*g2_h +   f2*g1_h +   f3*g0_h )
//
// <=>
//
// h = 2**0   * ( f0*g0_l + ( f1*g3_l +   f2*g2_l +   f3*g1_l ) * 38 ) +
//     2**64  * ( f0*g0_h + ( f1*g3_h +   f2*g2_h +   f3*g1_h ) * 38 ) +
//
//     2**64  * ( f0*g1_l +   f1*g0_l + ( f2*g3_l +   f3*g2_l ) * 38 ) +
//     2**128 * ( f0*g1_h +   f1*g0_h + ( f2*g3_h +   f3*g2_h ) * 38 ) +
//
//     2**128 * ( f0*g2_l +   f1*g1_l +   f2*g0_l + ( f3*g3_l ) * 38 ) +
//     2**192 * ( f0*g2_h +   f1*g1_h +   f2*g0_h + ( f3*g3_h ) * 38 ) +
//
//     2**192 * ( f0*g3_l +   f1*g2_l +   f2*g1_l +   f3*g0_l )
//     2**0   * ( f0*g3_h +   f1*g2_h +   f2*g1_h +   f3*g0_h ) * 38
//
// ...
//
// h = h' + r
//
// where
//
// h' =            2**0*h0 + 2**64*h1 + 2**128*h2 + 2**192*h3
// r  = 38     * ( 2**0*r0 + 2**64*r1 + 2**128*r2 + 2**192*r3 )
//
// but also
//
// r  = 2**256 * ( 2**0*r0 + 2**64*r1 + 2**128*r2 + 2**192*r3 )
//
// now the h0..3/r0..3
//
//   h0 = ( f0*g0_l )
//   h1 = ( f0*g0_h  + f0*g1_l + f1*g0_l )
//   h2 = ( f0*g1_h  + f0*g2_l + f1*g0_h  + f1*g1_l + f2*g0_l )
//   h3 = ( f0*g2_h  + f0*g3_l + f1*g1_h  + f1*g2_l + f2*g0_h  + f2*g1_l + f3*g0_l )
//   r0 = ( f0*g3_h  + f1*g2_h + f1*g3_l  + f2*g1_h + f2*g2_l  + f3*g0_h + f3*g1_l )
//   r1 = ( f1*g3_h  + f2*g2_h + f2*g3_l  + f3*g1_h + f3*g2_l )
//   r2 = ( f2*g3_h  + f3*g2_h + f3*g3_l )
//   r3 = ( f3*g3_h )
//
// and rearranged:
//
//   h0 = ( f0*g0_l )
//   h1 = ( f0*g0_h + f0*g1_l + f1*g0_l )
//   h2 = ( f0*g1_h + f0*g2_l + f1*g0_h + f1*g1_l + f2*g0_l )
//   h3 = ( f0*g2_h + f0*g3_l + f1*g1_h + f1*g2_l + f2*g0_h + f2*g1_l + f3*g0_l )
//   r0 = ( f0*g3_h +           f1*g2_h + f1*g3_l + f2*g1_h + f2*g2_l + f3*g0_h + f3*g1_l )
//   r1 = (                     f1*g3_h +           f2*g2_h + f2*g3_l + f3*g1_h + f3*g2_l )
//   r2 = (                                         f2*g3_h +           f3*g2_h + f3*g3_l )
//   r3 = (                                                             f3*g3_h )
//
// rearranged again (by columns):
//
//   h0  = ( f0*g0_l )
//   h1  = ( f0*g0_h + f0*g1_l )
//   h2  = ( f0*g1_h + f0*g2_l )
//   h3  = ( f0*g2_h + f0*g3_l )
//   r0  = ( f0*g3_h )
//
//   h1 += ( f1*g0_l )
//   h2 += ( f1*g0_h + f1*g1_l )
//   h3 += ( f1*g1_h + f1*g2_l )
//   r0 += ( f1*g2_h + f1*g3_l )
//   r1  = ( f1*g3_h )
//
//   h2 += ( f2*g0_l )
//   h3 += ( f2*g0_h + f2*g1_l)
//   r0 += ( f2*g1_h + f2*g2_l)
//   r1 += ( f2*g2_h + f2*g3_l)
//   r2  = ( f2*g3_h )
//
//   h3 += ( f3*g0_l )
//   r0 += ( f3*g0_h + f3*g1_l )
//   r1 += ( f3*g1_h + f3*g2_l )
//   r2 += ( f3*g2_h + f3*g3_l )
//   r3  = ( f3*g3_h )
//
// more flattening
//
//   h0  = ( f0*g0_l )
//   h1  = ( f0*g0_h ) 
//   h1 += ( f0*g1_l )
//   h2  = ( f0*g1_h )
//   h2 += ( f0*g2_l )
//   h3  = ( f0*g2_h )
//   h3 += ( f0*g3_l )
//   r0  = ( f0*g3_h )
//
//   h1 += ( f1*g0_l )
//   h2 += ( f1*g0_h )
//   h2 += ( f1*g1_l )
//   h3 += ( f1*g1_h )
//   h3 += ( f1*g2_l )
//   r0 += ( f1*g2_h )
//   r0 += ( f1*g3_l )
//   r1  = ( f1*g3_h )
//
//   h2 += ( f2*g0_l )
//   h3 += ( f2*g0_h )
//   h3 += ( f2*g1_l )
//   r0 += ( f2*g1_h )
//   r0 += ( f2*g2_l )
//   r1 += ( f2*g2_h )
//   r1 += ( f2*g3_l )
//   r2  = ( f2*g3_h )
//
//   h3 += ( f3*g0_l )
//   r0 += ( f3*g0_h )
//   r0 += ( f3*g1_l )
//   r1 += ( f3*g1_h )
//   r1 += ( f3*g2_l )
//   r2 += ( f3*g2_h )
//   r2 += ( f3*g3_l )
//   r3  = ( f3*g3_h )
//
// now with carry / overflow flags
//
//   cf, of  = 0, 0
//       h0  = ( f0*g0_l )
//       h1  = ( f0*g0_h ) 
//   cf, h1 += ( f0*g1_l ) + cf
//       h2  = ( f0*g1_h )
//   cf, h2 += ( f0*g2_l ) + cf
//       h3  = ( f0*g2_h )
//   cf, h3 += ( f0*g3_l ) + cf
//       r0  = ( f0*g3_h )
//   cf, r0 += ( 0       ) + cf // f0*g3 <= 2**64-1 * 2**64-1
//                              //          2**128 - 2**65 + 1
//                              // in this case there is at least one bit in r0
//                              // which is zero. so there is no overflow / 
//                              // carry flag : cf = 0
//
//   cf, h1 += ( f1*g0_l ) + cf
//   cf, h2 += ( f1*g0_h ) + cf
//   of, h2 += ( f1*g1_l ) + of
//   cf, h3 += ( f1*g1_h ) + cf
//   of, h3 += ( f1*g2_l ) + of
//   cf, r0 += ( f1*g2_h ) + cf
//   of, r0 += ( f1*g3_l ) + of
//       r1  = ( f1*g3_h )      
//   cf, r1 += ( 0       ) + cf // CHECK NOTE1
//   of, r1 += ( 0       ) + of // 
//
//////
// NOTE1:
// - worst case scenario (x=1 and y=3 for the last block):
//   fx and gy = 2**64-1
//   so fx*gy = 2**128 - 2**65 + 1
//            = 0xfffffffffffffffe_0000000000000001
//
//   fx*gy_l  = 0x0000000000000001
//   fx*gy_h  = 0xfffffffffffffffe
//
//   TODO
//
//////
// 
//   cf, h2 += ( f2*g0_l ) + cf
//   cf, h3 += ( f2*g0_h ) + cf
//   of, h3 += ( f2*g1_l ) + of
//   cf, r0 += ( f2*g1_h ) + cf
//   of, r0 += ( f2*g2_l ) + of
//   cf, r1 += ( f2*g2_h ) + cf
//   of, r1 += ( f2*g3_l ) + of
//       r2  = ( f2*g3_h )
//   cf, r2 += ( 0       ) + cf // CHECK NOTE1
//   of, r2 += ( 0       ) + of //
//
//   cf, h3 += ( f3*g0_l ) + cf
//   cf, r0 += ( f3*g0_h ) + cf
//   of, r0 += ( f3*g1_l ) + of
//   cf, r1 += ( f3*g1_h ) + cf
//   of, r1 += ( f3*g2_l ) + of
//   cf, r2 += ( f3*g2_h ) + cf
//   of, r2 += ( f3*g3_l ) + of
//       r3  = ( f3*g3_h )
//   cf, r3 += ( 0       ) + cf // CHECK NOTE1
//   of, r3 += ( 0       ) + of //
//
//
// now with temporary variables
//
//   cf, of  = 0, 0
//   h1, h0  = ( f0*g0   )
//
//   h2, t1  = ( f0*g1   )
//   cf, h1 += ( t1      ) + cf
//
//   h3, t2  = ( f0*g2   )
//   cf, h2 += ( t2      ) + cf
//
//   r0, t3  = ( f0*g3   )
//   cf, h3 += ( t3      ) + cf
//
//   cf, r0 += ( 0       ) + cf // cf = 0
//
//////
//
//   t2, t1  = ( f1*g0   )
//   cf, h1 += ( t1      ) + cf
//   cf, h2 += ( t2      ) + cf
//
//   t3, t2  = ( f1*g1   )
//   of, h2 += ( t2      ) + of
//   cf, h3 += ( t3      ) + cf
//
//   t0, t3  = ( f1*g2   )
//   of, h3 += ( t3      ) + of
//   cf, r0 += ( t0      ) + cf
//
//   r1, t0  = ( f1*g3   )
//   of, r0 += ( t0      ) + of
//
//   cf, r1 += ( 0       ) + cf // cf = 0
//   of, r1 += ( 0       ) + of // of = 0
//
//////
//
//   t3, t2  = ( f2*g0   )
//   cf, h2 += ( t2      ) + cf
//   cf, h3 += ( t3      ) + cf
//
//   t0, t3  = ( f2*g1   )
//   of, h3 += ( t3      ) + of
//   cf, r0 += ( t0      ) + cf
//
//   t1, t0  = ( f2*g2   )
//   of, r0 += ( t0      ) + of
//   cf, r1 += ( t1      ) + cf
//
//   r2, t1  = ( f2*g3   )
//   of, r1 += ( t1      ) + of
//
//   cf, r2 += ( 0       ) + cf // cf = 0 (?)
//   of, r2 += ( 0       ) + of // of = 0 (?)
//
//////
//
//   t0, t3  = ( f3*g0   )
//   cf, h3 += ( t3      ) + cf
//   cf, r0 += ( t0      ) + cf
//
//   t1, t0  = ( f3*g1   )
//   of, r0 += ( t0      ) + of
//   cf, r1 += ( t1      ) + cf
//
//   t2, t1  = ( f3*g2   )
//   of, r1 += ( t1      ) + of
//   cf, r2 += ( t2      ) + cf
//
//   r3, t2  = ( f3*g3   )
//   of, r2 += ( t2      ) + of
//
//   cf, r3 += ( 0       ) + cf // cf = 0 (?)
//   of, r3 += ( 0       ) + of // of = 0 (?)
//
// we can split this computation in 2 functions (by merging h and r into a big
// register array called h where r0 is at position 4, and so on...):
// - fe_mul_c0: for the first column
// - fe_mul_cn: for the remaining columns


fn fe64_mul_c0
( reg u64[8] h,
  reg u64 f0,
  reg u64[4] g,
  reg u64 z, // zero
  reg bool cf of // cf = 0 and of = 0
) -> reg u64[8], reg bool, reg bool
{
  inline int i;
  reg u64 hi lo;

  (h[1], h[0]) = #x86_MULX ( f0, g[0] );

  for i=1 to 4
  { ( h[i+1], lo ) = #x86_MULX ( f0, g[i] );
      cf, h[i]     = #x86_ADCX ( h[i], lo, cf );
  }

  cf, h[4] = #x86_ADCX ( h[4], z, cf ); // cf = 0

  return h, cf, of;
}


fn fe64_mul_cn
( reg u64[8] h,
  reg u64 f,
  reg u64[4] g,
  reg u64 z, // zero
  inline int cn, // column index 1 <= i <= 3 
  reg bool cf of // cf = 0 and of = 0
) -> reg u64[8], reg bool, reg bool
{
  inline int i;
  reg u64 hi lo;

  for i=0 to 3
  { ( hi, lo )    = #x86_MULX ( f, g[i] );
    of, h[i  +cn] = #x86_ADOX ( h[i  +cn], lo, of );
    cf, h[i+1+cn] = #x86_ADCX ( h[i+1+cn], hi, cf );
  }

  ( h[4+cn], lo ) = #x86_MULX ( f, g[3] );
  of, h[3+cn]     = #x86_ADOX ( h[3+cn], lo, of);

  cf, h[4+cn]     = #x86_ADCX ( h[4+cn], z, cf);
  of, h[4+cn]     = #x86_ADOX ( h[4+cn], z, of);

  return h, cf, of;
}


// TODO f_mem will change to stack u64[4] when the implementation is complete
// the register that this pointer needs is very valuable 
fn _fe64_mul
( reg u64 f_mem,
  reg u64[4] g
) -> reg u64[8]
{
  inline int n;
  reg bool cf of;
  reg u64[8] h;
  reg u64 _38 f z;

  z = 0; // CHECKME
  of, cf, _, _, _, z = #x86_XOR(z, z);

  f = [f_mem + 8*0];
  h, cf, of = fe64_mul_c0(h, f, g, z, cf, of);

  for n=1 to 4
  { f = [f_mem + 8*n];
    h, cf, of = fe64_mul_cn(h, f, g, z, n, cf, of);
  }

  _38 = 38;
  h = fe64_reduce(h, _38, z, cf, of);
  
  return h;
}
#endif
