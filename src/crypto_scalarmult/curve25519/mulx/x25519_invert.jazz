#ifndef CRYPTO_SCALARMULT_X25519_MULX_INVERT
#define CRYPTO_SCALARMULT_X25519_MULX_INVERT

#include "crypto_scalarmult/curve25519/mulx/x25519_sqr.jazz"
#include "crypto_scalarmult/curve25519/mulx/x25519_mul.jazz"

// supercop * / crypto_scalarmult / curve25519 ref10 implementation
inline fn __fe64_invert(reg u64[4] f) -> reg u64[4]
{
  reg u32 i;
  stack u64[4] fs t0s t1s t2s t3s;
  reg u64[4] t0  t1  t2  t3;

  fs = f;

  // z2 = z1^2^1
  t0  = __fe64_sqr_rr(f);
  t0s = t0;

  // z8 = z2^2^2
  t1  = __fe64_sqr_rr(t0);
  t1  = __fe64_sqr_rr(t1);

  // z9 = z1*z8
  t1  = __fe64_mul_rsr(fs,t1);
  t1s = t1;

  // z11 = z2*z9
  t0  = __fe64_mul_rsr(t0s,t1);
  t0s = t0;

  // z22 = z11^2^1
  t2 = __fe64_sqr_rr(t0);

  // z_5_0 = z9*z22
  t1  = __fe64_mul_rsr(t1s,t2);
  t1s = t1;

  // z_10_5 = z_5_0^2^5
  t2 = __fe64_sqr_rr(t1);
  i = 4/2;
  t2 = __fe64_it_sqr_x2(i, t2); 
  t2s = t2;

  // z_10_0 = z_10_5*z_5_0
  t1  = __fe64_mul_rsr(t1s,t2);
  t1s = t1;

  // z_20_10 = z_10_0^2^10
  i = 10/2;
  t2 = __fe64_it_sqr_x2(i, t1);

  // z_20_0 = z_20_10*z_10_0
  t2  = __fe64_mul_rsr(t1s,t2);
  t2s = t2;

  // z_40_20 = z_20_0^2^20
  i = 20/2;
  t3 = __fe64_it_sqr_x2(i, t2);

  // z_40_0 = z_40_20*z_20_0
  t2  = __fe64_mul_rsr(t2s,t3);

  // z_50_10 = z_40_0^2^10
  i = 10/2;
  t2 = __fe64_it_sqr_x2(i, t2);

  // z_50_0 = z_50_10*z_10_0
  t1  = __fe64_mul_rsr(t1s,t2);
  t1s = t1;

  // z_100_50 = z_50_0^2^50
  i = 50/2;
  t2 = __fe64_it_sqr_x2(i, t1);

  // z_100_0 = z_100_50*z_50_0
  t2  = __fe64_mul_rsr(t1s,t2);
  t2s = t2;

  // z_200_100 = z_100_0^2^100
  i = 100/2;
  t3 = __fe64_it_sqr_x2(i, t2);

  // z_200_0 = z_200_100*z_100_0
  t2  = __fe64_mul_rsr(t2s,t3);

  // z_250_50 = z_200_0^2^50
  i = 50/2;
  t2 = __fe64_it_sqr_x2(i, t2);

  // z_250_0 = z_250_50*z_50_0
  t1  = __fe64_mul_rsr(t1s,t2);

  // z_255_5 = z_250_0^2^5
  i = 4/2;
  t1 = __fe64_it_sqr_x2(i, t1);
  t1 = __fe64_sqr_rr(t1);

  // z_255_21 = z_255_5*z11
  t1 = __fe64_mul_rsr(t0s,t1);

  return t1;
}

#endif
